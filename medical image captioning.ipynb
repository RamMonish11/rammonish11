{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964670b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Kaggle-compatible paths\n",
    "BASE_DIR = \"/kaggle/working\" if os.path.exists(\"/kaggle\") else os.path.dirname(os.path.abspath(__file__))\n",
    "INPUT_DIR = \"/kaggle/input\" if os.path.exists(\"/kaggle\") else os.path.join(BASE_DIR, \"input\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'base_dir': BASE_DIR,\n",
    "    'input_dir': INPUT_DIR,\n",
    "    'data_dir': os.path.join(INPUT_DIR, r\"C:\\Users\\rammo\\OneDrive\\Desktop\\medical-captioning\\data\"),  # Adjust to your dataset name\n",
    "\n",
    "    'models_dir': os.path.join(BASE_DIR, \"models\"),\n",
    "    'cache_dir': os.path.join(BASE_DIR, \"cache\"),\n",
    "    'embed_size': 256,\n",
    "    'hidden_size': 512,\n",
    "    'max_len': 20,\n",
    "    'batch_size': 16,  \n",
    "    'epochs': 10,       \n",
    "    'lr': 1e-3,\n",
    "    'image_size': (224, 224)\n",
    "}\n",
    "\n",
    "# Auto-derived paths\n",
    "PATHS = {\n",
    "    'images': os.path.join(CONFIG['data_dir'], \"images\"),\n",
    "    'xml': os.path.join(CONFIG['data_dir'], \"xml\"),\n",
    "    'annotations': os.path.join(CONFIG['base_dir'], \"annotations.csv\"),\n",
    "    'clean_annotations': os.path.join(BASE_DIR, \"annotations_clean.csv\"),\n",
    "    'encoder_model': os.path.join(CONFIG['models_dir'], \"encoder.pth\"),\n",
    "    'decoder_model': os.path.join(CONFIG['models_dir'], \"decoder.pth\"),\n",
    "    'tokenizer_cache': CONFIG['cache_dir'],\n",
    "    'submission': os.path.join(BASE_DIR, \"submission.csv\")\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [CONFIG['models_dir'], CONFIG['cache_dir']]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handles all data preparation tasks.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sample_data():\n",
    "        \"\"\"Create sample data when no source files exist.\"\"\"\n",
    "        sample_data = pd.DataFrame({\n",
    "            'image_id': ['sample.png'],\n",
    "            'caption': ['Sample medical image']\n",
    "        })\n",
    "        sample_data.to_csv(PATHS['clean_annotations'], index=False)\n",
    "        print(\"‚úÖ Sample data created\")\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_from_xml():\n",
    "        \"\"\"Extract captions from XML files.\"\"\"\n",
    "        if not os.path.exists(PATHS['xml']):\n",
    "            print(\"No XML directory found\")\n",
    "            return\n",
    "            \n",
    "        rows = []\n",
    "        for xml_file in os.listdir(PATHS['xml']):\n",
    "            if xml_file.endswith(\".xml\"):\n",
    "                xml_path = os.path.join(PATHS['xml'], xml_file)\n",
    "                try:\n",
    "                    parser = ET.XMLParser()\n",
    "                    tree = ET.parse(xml_path, parser)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    findings = []\n",
    "                    for elem in root.findall(\".//AbstractText\"):\n",
    "                        label = elem.attrib.get(\"Label\", \"\").lower()\n",
    "                        if label in [\"findings\", \"impression\"] and elem.text:\n",
    "                            findings.append(elem.text.strip())\n",
    "                    \n",
    "                    caption_text = \" \".join(findings) if findings else \"No findings available\"\n",
    "                    \n",
    "                    for parent_img in root.findall(\".//parentImage\"):\n",
    "                        img_id = parent_img.attrib.get(\"id\") + \".png\"\n",
    "                        rows.append([img_id, caption_text])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {xml_file}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if rows:\n",
    "            with open(PATHS['annotations'], \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"image_id\", \"caption\"])\n",
    "                writer.writerows(rows)\n",
    "            print(f\"‚úÖ Extracted {len(rows)} annotations from XML\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_annotations():\n",
    "        \"\"\"Clean and validate annotations.\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(PATHS['annotations'])\n",
    "        except FileNotFoundError:\n",
    "            DataProcessor.create_sample_data()\n",
    "            return\n",
    "        \n",
    "        df['image_id'] = df['image_id'].str.strip().str.replace('.jpg', '.png', regex=False)\n",
    "        df[\"image_path\"] = df[\"image_id\"].apply(lambda x: os.path.join(PATHS['images'], x))\n",
    "        df = df[df[\"image_path\"].apply(os.path.exists)]\n",
    "        \n",
    "        df.to_csv(PATHS['clean_annotations'], index=False)\n",
    "        print(f\"‚úÖ Cleaned annotations: {len(df)} valid rows\")\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare():\n",
    "        \"\"\"Main data preparation pipeline.\"\"\"\n",
    "        print(\"Starting data preparation...\")\n",
    "        print(f\"Looking for data in: {CONFIG['data_dir']}\")\n",
    "        \n",
    "        if os.path.exists(PATHS['xml']) and os.listdir(PATHS['xml']):\n",
    "            DataProcessor.extract_from_xml()\n",
    "        \n",
    "        DataProcessor.clean_annotations()\n",
    "\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    \"\"\"Dataset for image captioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, img_folder, tokenizer, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.img_folder = img_folder\n",
    "        \n",
    "        self.image_paths = [os.path.join(img_folder, fname) for fname in self.data['image_id']]\n",
    "        self.captions = self.data['caption'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            with Image.open(self.image_paths[idx]) as img:\n",
    "                image = img.convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            image = torch.zeros(3, *CONFIG['image_size'])\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            self.captions[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=CONFIG['max_len'],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return image, encodings[\"input_ids\"].squeeze(0)\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    \"\"\"CNN encoder using ResNet50.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights='DEFAULT')\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        resnet.fc = nn.Linear(resnet.fc.in_features, CONFIG['embed_size'])\n",
    "        self.model = resnet\n",
    "    \n",
    "    def forward(self, images):\n",
    "        return self.model(images)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"RNN decoder using LSTM.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, CONFIG['embed_size'])\n",
    "        self.lstm = nn.LSTM(CONFIG['embed_size'], CONFIG['hidden_size'], batch_first=True)\n",
    "        self.fc = nn.Linear(CONFIG['hidden_size'], vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions[:, :-1])\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        return self.fc(outputs)[:, 1:, :]\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Handles model training and inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.tokenizer = self._setup_tokenizer()\n",
    "        self.transform = self._setup_transforms()\n",
    "        self.train_losses = []\n",
    "        \n",
    "    def _setup_tokenizer(self):\n",
    "\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                \"gpt2\",\n",
    "                cache_dir=PATHS.get('tokenizer_cache', None),\n",
    "                legacy=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading tokenizer: {e}\")\n",
    "            print(\"üîÅ Falling back to local GPT-2 tokenizer...\")\n",
    "    \n",
    "            try:\n",
    "                local_dir = \"/kaggle/working/gpt2_tokenizer\"\n",
    "                if not os.path.exists(local_dir):\n",
    "                    os.makedirs(local_dir, exist_ok=True)\n",
    "                    print(\"üì¶ Downloading GPT-2 tokenizer locally...\")\n",
    "                    subprocess.run([\n",
    "                        \"huggingface-cli\", \"download\", \"gpt2\",\n",
    "                        \"--local-dir\", local_dir,\n",
    "                        \"--repo-type\", \"model\"\n",
    "                    ], check=True)\n",
    "    \n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    local_dir,\n",
    "                    legacy=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Fallback tokenizer load failed: {e2}\")\n",
    "                raise RuntimeError(\"Tokenizer could not be loaded. Check internet connection or model availability.\")\n",
    "            \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        return tokenizer\n",
    "    \n",
    "    def _setup_transforms(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(CONFIG['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])   \n",
    "            \n",
    "        ])\n",
    "    \n",
    "    def _collate_fn(self, batch):\n",
    "        images, captions = zip(*batch)\n",
    "        return torch.stack(images, dim=0), torch.stack(captions, dim=0)\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        return CaptionDataset(\n",
    "            PATHS['clean_annotations'],\n",
    "            PATHS['images'],\n",
    "            self.tokenizer,\n",
    "            self.transform\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "        dataset = self.create_dataset()\n",
    "        if len(dataset) == 0:\n",
    "            print(\"No valid data found.\")\n",
    "            return None, None\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=min(CONFIG['batch_size'], len(dataset)),\n",
    "            shuffle=True,\n",
    "            collate_fn=self._collate_fn,\n",
    "            num_workers=0  # Kaggle compatibility\n",
    "        )\n",
    "        \n",
    "        vocab_size = len(self.tokenizer)\n",
    "        encoder = EncoderCNN().to(self.device)\n",
    "        decoder = DecoderRNN(vocab_size).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        params = list(decoder.parameters()) + list(encoder.model.fc.parameters())\n",
    "        optimizer = torch.optim.Adam(params, lr=CONFIG['lr'])\n",
    "        \n",
    "        print(f\"Training on {self.device} with vocab size: {vocab_size}\")\n",
    "        \n",
    "        for epoch in range(CONFIG['epochs']):\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "            for images, captions in pbar:\n",
    "                images, captions = images.to(self.device), captions.to(self.device)\n",
    "                \n",
    "                features = encoder(images)\n",
    "                outputs = decoder(features, captions)\n",
    "                \n",
    "                targets = captions[:, 1:].contiguous().view(-1)\n",
    "                outputs = outputs.contiguous().view(-1, vocab_size)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_loss = epoch_loss / len(dataloader)\n",
    "            self.train_losses.append(avg_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{CONFIG['epochs']}] Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return encoder, decoder\n",
    "    \n",
    "    def generate_caption(self, image_path, encoder, decoder):\n",
    "        \"\"\"Generate caption for a single image.\"\"\"\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "                image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return \"Error loading image\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = encoder(image)\n",
    "            inputs = torch.zeros(1, CONFIG['max_len'], dtype=torch.long).to(self.device)\n",
    "            inputs[0, 0] = self.tokenizer.bos_token_id if self.tokenizer.bos_token_id else 0\n",
    "            \n",
    "            generated_ids = []\n",
    "            \n",
    "            for i in range(1, CONFIG['max_len']):\n",
    "                # Decoder needs at least 2 tokens (slices [:, :-1])\n",
    "                outputs = decoder(features, inputs[:, :i+1])\n",
    "                if outputs.size(1) > 0:\n",
    "                    predicted_id = outputs[0, -1, :].argmax().item()\n",
    "                    generated_ids.append(predicted_id)\n",
    "                    inputs[0, i] = predicted_id\n",
    "                    \n",
    "                    if predicted_id == self.tokenizer.eos_token_id:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            return self.tokenizer.decode(generated_ids, skip_special_tokens=True) if generated_ids else \"Unable to generate caption\"\n",
    "    \n",
    "    def visualize_training(self):\n",
    "        \"\"\"Plot training loss curve.\"\"\"\n",
    "        if not self.train_losses:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(self.train_losses) + 1), self.train_losses, marker='o', linewidth=2)\n",
    "        plt.title('Training Loss Over Epochs', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Average Loss', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(BASE_DIR, 'training_loss.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"‚úÖ Training loss plot saved\")\n",
    "    \n",
    "    def visualize_predictions(self, encoder, decoder, num_samples=6):\n",
    "        \"\"\"Visualize sample predictions.\"\"\"\n",
    "        dataset = self.create_dataset()\n",
    "        if len(dataset) == 0:\n",
    "            return\n",
    "        \n",
    "        indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, ax in zip(indices, axes):\n",
    "            img_path = dataset.image_paths[idx]\n",
    "            true_caption = dataset.captions[idx]\n",
    "            pred_caption = self.generate_caption(img_path, encoder, decoder)\n",
    "            \n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"True: {true_caption[:40]}...\\nPred: {pred_caption[:40]}...\", \n",
    "                        fontsize=9, wrap=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(BASE_DIR, 'predictions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"‚úÖ Predictions visualization saved\")\n",
    "    \n",
    "    def visualize_dataset_stats(self):\n",
    "        \"\"\"Visualize dataset statistics.\"\"\"\n",
    "        dataset = self.create_dataset()\n",
    "        if len(dataset) == 0:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Caption length distribution\n",
    "        caption_lengths = [len(cap.split()) for cap in dataset.captions]\n",
    "        axes[0, 0].hist(caption_lengths, bins=30, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_title('Caption Length Distribution', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Number of Words')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Word frequency\n",
    "        all_words = ' '.join(dataset.captions).lower().split()\n",
    "        word_freq = Counter(all_words).most_common(20)\n",
    "        words, counts = zip(*word_freq)\n",
    "        axes[0, 1].barh(range(len(words)), counts, color='coral')\n",
    "        axes[0, 1].set_yticks(range(len(words)))\n",
    "        axes[0, 1].set_yticklabels(words)\n",
    "        axes[0, 1].set_title('Top 20 Most Common Words', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Frequency')\n",
    "        axes[0, 1].invert_yaxis()\n",
    "        \n",
    "        # Dataset size\n",
    "        axes[1, 0].bar(['Total Images'], [len(dataset)], color='lightgreen', edgecolor='black')\n",
    "        axes[1, 0].set_title('Dataset Size', fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Caption character distribution\n",
    "        char_lengths = [len(cap) for cap in dataset.captions]\n",
    "        axes[1, 1].hist(char_lengths, bins=30, color='plum', edgecolor='black')\n",
    "        axes[1, 1].set_title('Caption Character Length Distribution', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Number of Characters')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(BASE_DIR, 'dataset_stats.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"‚úÖ Dataset statistics visualization saved\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        print(\"üè• Medical Image Captioning - Kaggle Version\")\n",
    "        print(f\"Base directory: {BASE_DIR}\")\n",
    "        print(f\"Input directory: {INPUT_DIR}\")\n",
    "        \n",
    "        DataProcessor.prepare()\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        trainer = ModelTrainer(device)\n",
    "        \n",
    "        # Visualize dataset before training\n",
    "        print(\"\\nüìä Generating dataset visualizations...\")\n",
    "        trainer.visualize_dataset_stats()\n",
    "        \n",
    "        encoder, decoder = trainer.train()\n",
    "        if encoder and decoder:\n",
    "            torch.save(encoder.state_dict(), PATHS['encoder_model'])\n",
    "            torch.save(decoder.state_dict(), PATHS['decoder_model'])\n",
    "            print(f\"‚úÖ Models saved to: {CONFIG['models_dir']}\")\n",
    "            \n",
    "            # Generate visualizations\n",
    "            print(\"\\nüìä Generating visualizations...\")\n",
    "            trainer.visualize_training()\n",
    "            trainer.visualize_predictions(encoder, decoder)\n",
    "            \n",
    "            # Test caption generation\n",
    "            dataset = trainer.create_dataset()\n",
    "            if len(dataset) > 0:\n",
    "                test_path = dataset.image_paths[0]\n",
    "                if os.path.exists(test_path):\n",
    "                    caption = trainer.generate_caption(test_path, encoder, decoder)\n",
    "                    print(f\"Generated caption: {caption}\")\n",
    "                    \n",
    "                    import shutil\n",
    "                    models_dir = CONFIG.get(\"models_dir\")\n",
    "                    \n",
    "                    if models_dir and os.path.exists(models_dir):\n",
    "                        # Remove trailing slash if present (prevents weird double paths)\n",
    "                        models_dir = models_dir.rstrip(\"/\")\n",
    "                    \n",
    "                        shutil.make_archive(BASE_DIR, \"zip\", models_dir)\n",
    "                        print(f\"üì¶ Successfully zipped model directory ‚Üí {models_dir}.zip\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Model directory not found or not set: {models_dir}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
