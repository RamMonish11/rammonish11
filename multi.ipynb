{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b6f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e7fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f81b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee’s Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d8a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 75000 entries, 0 to 74999\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sample_id        75000 non-null  int64  \n",
      " 1   catalog_content  75000 non-null  object \n",
      " 2   image_link       75000 non-null  object \n",
      " 3   price            75000 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f37702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>75000.000000</td>\n",
       "      <td>75000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>149841.917707</td>\n",
       "      <td>23.647654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>86585.346513</td>\n",
       "      <td>33.376932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>73845.750000</td>\n",
       "      <td>6.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>150129.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>225040.250000</td>\n",
       "      <td>28.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>299438.000000</td>\n",
       "      <td>2796.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sample_id         price\n",
       "count   75000.000000  75000.000000\n",
       "mean   149841.917707     23.647654\n",
       "std     86585.346513     33.376932\n",
       "min         0.000000      0.130000\n",
       "25%     73845.750000      6.795000\n",
       "50%    150129.000000     14.000000\n",
       "75%    225040.250000     28.625000\n",
       "max    299438.000000   2796.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1740fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_id          0\n",
      "catalog_content    0\n",
      "image_link         0\n",
      "price              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89169d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item Name: La Victoria Green Taco Sauce Mild, 12 Ounce (Pack of 6)\\nValue: 72.0\\nUnit: Fl Oz\\n'\n",
      " 'Item Name: Salerno Cookies, The Original Butter Cookies, 8 Ounce (Pack of 4)\\nBullet Point 1: Original Butter Cookies: Classic butter cookies made with real butter\\nBullet Point 2: Variety Pack: Includes 4 boxes with 32 cookies total\\nBullet Point 3: Occasion Perfect: Delicious cookies for birthdays, weddings, anniversaries\\nBullet Point 4: Shareable Treats: Fun to give and enjoy with friends and family\\nBullet Point 5: Salerno Brand: Trusted brand of delicious butter cookies since 1925\\nValue: 32.0\\nUnit: Ounce\\n'\n",
      " 'Item Name: Bear Creek Hearty Soup Bowl, Creamy Chicken with Rice, 1.9 Ounce (Pack of 6)\\nBullet Point 1: Loaded with hearty long grain wild rice and vegetables\\nBullet Point 2: Full of hearty goodness\\nBullet Point 3: Single serve bowls\\nBullet Point 4: Easy to prepare mix\\nBullet Point 5: 0 grams trans fat\\nValue: 11.4\\nUnit: Ounce\\n'\n",
      " ...\n",
      " 'Item Name: Jolly Rancher Hard Candy - Blue Raspberry - 5 Pound Resealable Bag\\nProduct Description: 5lb Blue Raspberry\\nValue: 80.0\\nUnit: Ounce\\n'\n",
      " \"Item Name: Nescafe Dolce Gusto Capsules - CARAMEL MACCHIATO, 16 Pods\\nBullet Point 1: Nescafe Dolce Gusto Caramel Latte Macchiato is an indulgent three-layered beverage of hot frothed milk, smooth espresso and a generous milk froth topping with a swirl of caramel flavor infused throughout\\nBullet Point 2: WHAT'S INSIDE - An irresistibly smooth latte swirled with sweet caramel flavors\\nBullet Point 3: COFFEEHOUSE-QUALITY DRINKS - Create professional quality specialty beverages, coffees and espresso with a rich velvety crema thanks to the machine's high pressure\\nBullet Point 4: EASY TO PREPARE - Simply pop in the white milk capsule and brew followed by the black espresso capsule and brew for a perfectly layered beverage\\nBullet Point 5: SATISFACTION GUARANTEED: Covered by the Nestlé Professional 60-Day Satisfaction Guarantee for Select Products. See below for details.\\nValue: 16.0\\nUnit: Count\\n\"\n",
      " 'Item Name: Pimenton de la Vera - Picante (2.47 ounce)\\nBullet Point 1: No hydrogenated fats or high fructose corn syrup allowed in any food\\nBullet Point 2: No bleached or bromated flour\\nBullet Point 3: No synthetic nitrates or nitrites\\nProduct Description: Baking\\nValue: 2.47\\nUnit: Ounce\\n']\n"
     ]
    }
   ],
   "source": [
    "print(df['catalog_content'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64907fe7",
   "metadata": {},
   "source": [
    "Textual data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0073f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185e71cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['sample_id', 'catalog_content', 'image_link', 'price']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc99f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample catalog content (first row):\n",
      " Item Name: La Victoria Green Taco Sauce Mild, 12 Ounce (Pack of 6)\n",
      "Value: 72.0\n",
      "Unit: Fl Oz\n",
      " \n",
      "\n",
      "Parsed sample:\n",
      " item_name        La Victoria Green Taco Sauce Mild, 12 Ounce (P...\n",
      "bullet_points                                                     \n",
      "value                                                         72.0\n",
      "unit                                                         Fl Oz\n",
      "dtype: object\n",
      "\n",
      "Parsed columns head:\n",
      "                                           item_name  \\\n",
      "0  La Victoria Green Taco Sauce Mild, 12 Ounce (P...   \n",
      "1  Salerno Cookies, The Original Butter Cookies, ...   \n",
      "2  Bear Creek Hearty Soup Bowl, Creamy Chicken wi...   \n",
      "3  Judee’s Blue Cheese Powder 11.25 oz - Gluten-F...   \n",
      "4  kedem Sherry Cooking Wine, 12.7 Ounce - 12 per...   \n",
      "\n",
      "                                       bullet_points  value   unit  price  \n",
      "0                                                     72.00  Fl Oz   4.89  \n",
      "1  Original Butter Cookies: Classic butter cookie...  32.00  Ounce  13.12  \n",
      "2  Loaded with hearty long grain wild rice and ve...  11.40  Ounce   1.97  \n",
      "3  Add to your favorite appetizers, dips & spread...  11.25  Ounce  30.34  \n",
      "4                                                     12.00  Count  66.49  \n",
      "\n",
      "Missing values after parse:\n",
      "item_name          0\n",
      "bullet_points      0\n",
      "value            940\n",
      "unit               0\n",
      "price              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def parse_catalog(text):\n",
    "    # handle missing\n",
    "    if pd.isna(text):\n",
    "        return pd.Series({\"item_name\":\"\", \"bullet_points\":\"\", \"value\":None, \"unit\":\"\"})\n",
    "    # ensure string\n",
    "    text = str(text)\n",
    "    # ITEM NAME\n",
    "    m_name = re.search(r'Item Name:\\s*(.*?)(?:\\n|$)', text, flags=re.IGNORECASE)\n",
    "    item_name = m_name.group(1).strip() if m_name else \"\"\n",
    "    # BULLET POINTS (all)\n",
    "    bullets = re.findall(r'Bullet Point \\d+:\\s*(.*?)(?:\\n|$)', text, flags=re.IGNORECASE)\n",
    "    bullet_points = \" \".join(b.strip() for b in bullets) if bullets else \"\"\n",
    "    # VALUE (numeric)\n",
    "    m_value = re.search(r'Value:\\s*([\\d.]+)', text, flags=re.IGNORECASE)\n",
    "    value = float(m_value.group(1)) if m_value else None\n",
    "    # UNIT\n",
    "    m_unit = re.search(r'Unit:\\s*(.*?)(?:\\n|$)', text, flags=re.IGNORECASE)\n",
    "    unit = m_unit.group(1).strip() if m_unit else \"\"\n",
    "    return pd.Series({\"item_name\": item_name, \"bullet_points\": bullet_points, \"value\": value, \"unit\": unit})\n",
    "\n",
    "# Debug: parse a single sample to ensure parser works\n",
    "sample_idx = 0\n",
    "print(\"Sample catalog content (first row):\\n\", df.loc[sample_idx, \"catalog_content\"][:1000], \"\\n\")\n",
    "print(\"Parsed sample:\\n\", parse_catalog(df.loc[sample_idx, \"catalog_content\"]))\n",
    "\n",
    "# Apply parser to entire column\n",
    "parsed = df['catalog_content'].apply(parse_catalog)\n",
    "df = pd.concat([df, parsed], axis=1)\n",
    "\n",
    "# Convert price to numeric if needed\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "# Show results\n",
    "print(\"\\nParsed columns head:\")\n",
    "print(df[['item_name','bullet_points','value','unit','price']].head())\n",
    "print(\"\\nMissing values after parse:\")\n",
    "print(df[['item_name','bullet_points','value','unit','price']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4840f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_idx = None\n",
    "for i, txt in enumerate(df['catalog_content']):\n",
    "    try:\n",
    "        parse_catalog(txt)\n",
    "    except Exception as e:\n",
    "        bad_idx = i\n",
    "        print(\"Failed at row:\", i, \"error:\", e)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e438a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rammo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rammo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  price\n",
      "0       la victoria green taco sauce mild ounce pack   4.89\n",
      "1  salerno cooky original butter cooky ounce pack...  13.12\n",
      "2  bear creek hearty soup bowl creamy chicken ric...   1.97\n",
      "3  judees blue cheese powder oz glutenfree nutfre...  30.34\n",
      "4           kedem sherry cooking wine ounce per case  66.49\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# initialize tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove punctuation, numbers, symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # remove stopwords & lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# combine both text columns into one\n",
    "df['combined_text'] = df['item_name'].fillna('') + ' ' + df['bullet_points'].fillna('')\n",
    "\n",
    "# clean the combined text\n",
    "df['clean_text'] = df['combined_text'].apply(clean_text)\n",
    "\n",
    "# display result\n",
    "print(df[['clean_text', 'price']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d5abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rammo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# full_multimodal_pipeline.py\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fefa8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da6fc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Preprocessing (log-price + value scaler + unit encoding)\n",
    "# ---------------------------\n",
    "# df should already have: 'clean_text', 'value', 'unit', 'price', 'image_link', 'sample_id'\n",
    "# If price has strings, make numeric:\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "df = df.dropna(subset=['price'])  # drop rows with nan price\n",
    "# Log-transform target for stability\n",
    "df['log_price'] = np.log1p(df['price'])\n",
    "\n",
    "# Value: numeric product measurement (we parsed earlier); fill NA with 0\n",
    "df['value'] = pd.to_numeric(df['value'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# Standard scale value\n",
    "value_scaler = StandardScaler()\n",
    "df['value_scaled'] = value_scaler.fit_transform(df[['value']])\n",
    "\n",
    "# Encode unit (categorical)\n",
    "unit_le = LabelEncoder()\n",
    "df['unit'] = df['unit'].fillna(\"unknown\")\n",
    "df['unit_idx'] = unit_le.fit_transform(df['unit'])\n",
    "\n",
    "# Train / validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=SEED)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36a95fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Tokenizer and image transforms\n",
    "# ---------------------------\n",
    "TEXT_MODEL = \"distilbert-base-uncased\"   # lightweight; change to \"bert-base-uncased\" if you want\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)\n",
    "MAX_TEXT_LEN = 64\n",
    "\n",
    "# Image transforms (train vs val)\n",
    "IMG_SIZE = 224\n",
    "train_img_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_img_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c391e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 3) Dataset (with simple image caching)\n",
    "# ---------------------------\n",
    "IMAGE_CACHE_DIR = \"./image_cache\"\n",
    "os.makedirs(IMAGE_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=8)\n",
    "        resp.raise_for_status()\n",
    "        img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        img.save(save_path, format='JPEG', quality=85)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # print(\"Download failed:\", e)\n",
    "        return False\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=MAX_TEXT_LEN, img_transforms=None, cache_dir=IMAGE_CACHE_DIR):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.img_transforms = img_transforms\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _load_image(self, url, sample_id):\n",
    "        # local filename based on sample id\n",
    "        file_name = f\"{sample_id}.jpg\"\n",
    "        path = os.path.join(self.cache_dir, file_name)\n",
    "        # if already downloaded, open it\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                return img\n",
    "            except:\n",
    "                os.remove(path)  # corrupt, try re-download\n",
    "\n",
    "        # try download\n",
    "        ok = download_image(url, path)\n",
    "        if ok:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                return img\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # fallback: return a plain gray image\n",
    "        return Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (127,127,127))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sample_id = row['sample_id']\n",
    "        text = str(row.get('clean_text', \"\"))[:1000]\n",
    "        image_url = row.get('image_link', \"\")\n",
    "        value_scaled = float(row.get('value_scaled', 0.0))\n",
    "        unit_idx = int(row.get('unit_idx', 0))\n",
    "        log_price = float(row.get('log_price', 0.0))\n",
    "\n",
    "        # tokenize text (fixed-length)\n",
    "        toks = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = toks['input_ids'].squeeze(0)        # (max_len,)\n",
    "        attention_mask = toks['attention_mask'].squeeze(0)\n",
    "\n",
    "        # load image\n",
    "        img = self._load_image(image_url, sample_id)\n",
    "        if self.img_transforms:\n",
    "            img = self.img_transforms(img)  # tensor\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': img,\n",
    "            'value': torch.tensor(value_scaled, dtype=torch.float32),\n",
    "            'unit_idx': torch.tensor(unit_idx, dtype=torch.long),\n",
    "            'label': torch.tensor(log_price, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# create datasets & loaders\n",
    "train_ds = ProductDataset(train_df, tokenizer, img_transforms=train_img_transforms)\n",
    "val_ds   = ProductDataset(val_df, tokenizer, img_transforms=val_img_transforms)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9299845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1013 16:50:32.363000 27440 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "c:\\Users\\rammo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rammo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 4) Multimodal model\n",
    "# ---------------------------\n",
    "class MultimodalRegressor(nn.Module):\n",
    "    def __init__(self, text_model_name=TEXT_MODEL, image_model_name='resnet50',\n",
    "                 num_units=len(unit_le.classes_), unit_emb_dim=8, hidden_dim=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # Text encoder\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        text_hidden = self.text_encoder.config.hidden_size\n",
    "\n",
    "        # Image encoder: pretrained ResNet50 without final fc\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        # remove final fc\n",
    "        self.image_encoder = nn.Sequential(*list(backbone.children())[:-1])  # outputs (B, 2048, 1,1)\n",
    "        image_feat_dim = 2048\n",
    "\n",
    "        # unit embedding\n",
    "        self.unit_emb = nn.Embedding(num_units, unit_emb_dim)\n",
    "\n",
    "        # final head\n",
    "        total_dim = text_hidden + image_feat_dim + 1 + unit_emb_dim\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(total_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image, value, unit_idx):\n",
    "        # text -> mean pooling over tokens\n",
    "        text_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last_hidden = text_out.last_hidden_state          # (B, seq, H)\n",
    "        mask = attention_mask.unsqueeze(-1)               # (B, seq, 1)\n",
    "        sum_hidden = (last_hidden * mask).sum(1)          # (B, H)\n",
    "        denom = mask.sum(1).clamp(min=1e-9)\n",
    "        text_feat = sum_hidden / denom                    # (B, H)\n",
    "\n",
    "        # image -> flatten\n",
    "        img_feat = self.image_encoder(image)              # (B, 2048, 1,1)\n",
    "        img_feat = img_feat.view(img_feat.size(0), -1)    # (B, 2048)\n",
    "\n",
    "        unit_emb = self.unit_emb(unit_idx)                # (B, unit_emb_dim)\n",
    "\n",
    "        # concat (value is scalar)\n",
    "        x = torch.cat([text_feat, img_feat, value.unsqueeze(1), unit_emb], dim=1)\n",
    "        out = self.head(x).squeeze(1)                     # (B,)\n",
    "        return out\n",
    "\n",
    "model = MultimodalRegressor().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "714f92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 6) Utility: evaluate\n",
    "# ---------------------------\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            image = batch['image'].to(DEVICE)\n",
    "            value = batch['value'].to(DEVICE)\n",
    "            unit_idx = batch['unit_idx'].to(DEVICE)\n",
    "            label = batch['label'].to(DEVICE)\n",
    "\n",
    "            out = model(input_ids, attention_mask, image, value, unit_idx)\n",
    "            loss = criterion(out, label)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            preds.append(out.detach().cpu().numpy())\n",
    "            trues.append(label.detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    trues = np.concatenate(trues)\n",
    "    # convert back to price-space\n",
    "    price_preds = np.expm1(preds)\n",
    "    price_trues = np.expm1(trues)\n",
    "    rmse = math.sqrt(mean_squared_error(price_trues, price_preds))\n",
    "    mae = mean_absolute_error(price_trues, price_preds)\n",
    "    return np.mean(losses), rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846adab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rammo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# 7) Training loop\n",
    "# ---------------------------\n",
    "best_rmse = float('inf')\n",
    "save_path = \"best_multimodal.pth\"\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        image = batch['image'].to(DEVICE)\n",
    "        value = batch['value'].to(DEVICE)\n",
    "        unit_idx = batch['unit_idx'].to(DEVICE)\n",
    "        label = batch['label'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask, image, value, unit_idx)\n",
    "            loss = criterion(outputs, label)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch} step {step}/{len(train_loader)} loss={np.mean(epoch_losses):.4f}\")\n",
    "\n",
    "    avg_train_loss = np.mean(epoch_losses)\n",
    "    val_loss, val_rmse, val_mae = evaluate(model, val_loader)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"Epoch {epoch} finished in {elapsed:.1f}s - train_loss={avg_train_loss:.4f} val_loss={val_loss:.4f} val_rmse={val_rmse:.2f} val_mae={val_mae:.2f}\")\n",
    "\n",
    "    # save best\n",
    "    if val_rmse < best_rmse:\n",
    "        best_rmse = val_rmse\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'value_scaler': value_scaler,\n",
    "            'unit_le_classes': unit_le.classes_,\n",
    "            'tokenizer': tokenizer.name_or_path\n",
    "        }, save_path)\n",
    "        print(\"Saved best model:\", save_path)\n",
    "\n",
    "print(\"Training complete. Best RMSE:\", best_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
